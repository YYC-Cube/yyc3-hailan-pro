---
@file: 029-AI集成架构设计.md
@description: HaiLan Pro AI功能集成架构设计，包含LLM对接、LangChain集成、智能推荐系统等
@author: YanYuCloudCube Team
@version: v1.0.0
@created: 2026-01-26
@updated: 2026-01-26
@status: published
@tags: [HaiLan-Pro-架构设计],[]
---

> ***YanYuCloudCube***
> 言启象限 | 语枢未来
> ***Words Initiate Quadrants, Language Serves as Core for the Future***
> 万象归元于云枢 | 深栈智启新纪元
> ***All things converge in the cloud pivot; Deep stacks ignite a new era of intelligence***

---

# 029 AI集成架构设计

## 概述

本文档详细描述HaiLan Pro的AI功能集成架构设计，包括LLM对接、LangChain集成、智能推荐系统、RAG架构等核心AI能力，确保项目按照YYC³标准规范进行开发和实施。

## 核心内容

### 1. 背景与目标

#### 1.1 项目背景
HaiLan Pro (海蓝) 是新一代高端、私密、智能的情趣健康生活管理平台。项目基于「五高五标五化」理念，通过 PWA 技术结合 AI 智能辅助与物联网，为用户提供从生理健康到心理愉悦的全方位解决方案。

#### 1.2 AI核心价值
- **智能健康顾问**：基于LLM的生理健康与情感咨询AI助手
- **个性化推荐**：根据用户画像提供精准的商品与服务推荐
- **隐私保护对话**：端到端加密的AI对话，保障用户隐私
- **知识库增强**：RAG架构结合专业知识库，提供专业健康建议

#### 1.3 文档目标
- 规范AI集成架构相关的业务标准与技术落地要求
- 定义LLM对接、提示词工程、知识库管理的技术方案
- 保障AI模块开发、实施、运维的一致性与规范性

### 2. 设计原则

#### 2.1 五高原则
- **高可用性**：AI服务多模型备份，确保7x24小时服务可用
- **高性能**：流式响应、缓存优化，响应时间<2秒
- **高安全性**：对话数据加密存储，符合隐私保护规范
- **高扩展性**：模块化设计，支持快速接入新的AI能力
- **高可维护性**：统一的AI服务接口，便于监控和调试

#### 2.2 五标体系
- **标准化**：统一的AI服务API规范与数据格式
- **规范化**：提示词工程最佳实践与版本管理
- **自动化**：自动化的模型评估与A/B测试流程
- **智能化**：智能的上下文管理与意图识别
- **可视化**：AI对话质量监控与分析仪表板

#### 2.3 五化架构
- **流程化**：标准化的AI服务调用流程
- **文档化**：完善的提示词模板与API文档
- **工具化**：LangChain集成与工具链自动化
- **数字化**：AI服务指标量化与效果追踪
- **生态化**：多模型兼容与第三方AI能力集成

### 3. AI集成架构设计

#### 3.1 整体架构图

```
┌─────────────────────────────────────────────────────────────────┐
│                      HaiLan Pro AI架构                           │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│  ┌─────────────────────────────────────────────────────────┐   │
│  │                    AI应用层                               │   │
│  │  ┌──────────┐  ┌──────────┐  ┌──────────┐  ┌─────────┐ │   │
│  │  │健康顾问   │  │情感陪伴   │  │智能推荐   │  │设备诊断  │ │   │
│  │  │Chatbot   │  │Companion │  │Recommend │  │Diagnose │ │   │
│  │  └──────────┘  └──────────┘  └──────────┘  └─────────┘ │   │
│  └─────────────────────────────────────────────────────────┘   │
│                              │                                   │
│  ┌─────────────────────────────────────────────────────────┐   │
│  │                   LangChain层                             │   │
│  │  ┌──────────┐  ┌──────────┐  ┌──────────┐  ┌─────────┐ │   │
│  │  │Chains    │  │Agents    │  │Memory    │  │Tools    │ │   │
│  │  │链式调用   │  │智能体     │  │记忆管理   │  │工具集   │ │   │
│  │  └──────────┘  └──────────┘  └──────────┘  └─────────┘ │   │
│  └─────────────────────────────────────────────────────────┘   │
│                              │                                   │
│  ┌─────────────────────────────────────────────────────────┐   │
│  │                    AI服务层                               │   │
│  │  ┌──────────┐  ┌──────────┐  ┌──────────┐  ┌─────────┐ │   │
│  │  │LLM服务   │  │Embedding │  │RAG服务   │  │提示词   │ │   │
│  │  │多模型适配 │  │向量化     │  │检索增强   │  │管理     │ │   │
│  │  └──────────┘  └──────────┘  └──────────┘  └─────────┘ │   │
│  └─────────────────────────────────────────────────────────┘   │
│                              │                                   │
│  ┌─────────────────────────────────────────────────────────┐   │
│  │                    知识库层                               │   │
│  │  ┌──────────┐  ┌──────────┐  ┌──────────┐  ┌─────────┐ │   │
│  │  │专业知识库 │  │产品知识库 │  │FAQ库     │  │向量库   │ │   │
│  │  │Medical KB │  │Product KB│  │Common QA │  │Vector DB│ │   │
│  │  └──────────┘  └──────────┘  └──────────┘  └─────────┘ │   │
│  └─────────────────────────────────────────────────────────┘   │
│                              │                                   │
│  ┌─────────────────────────────────────────────────────────┐   │
│  │                    数据安全层                             │   │
│  │  ┌──────────┐  ┌──────────┐  ┌──────────┐  ┌─────────┐ │   │
│  │  │内容过滤   │  │PII脱敏   │  │对话加密   │  │审计日志 │ │   │
│  │  │Moderation │  │Masking   │  │Encryption│  │Audit    │ │   │
│  │  └──────────┘  └──────────┘  └──────────┘  └─────────┘ │   │
│  └─────────────────────────────────────────────────────────┘   │
└─────────────────────────────────────────────────────────────────┘
         │                                    │
    ┌────▼────┐                          ┌───▼────┐
    │Client   │                          │AI API  │
    └─────────┘                          │Provider│
                                         └────────┘
```

#### 3.2 LLM服务层设计

##### 3.2.1 多模型适配器

```typescript
// src/ai/llm/adapters.ts
export enum LLMProvider {
  OPENAI = 'openai',
  ANTHROPIC = 'anthropic',
  QWEN = 'qwen',
  DEEPSEEK = 'deepseek',
  ZHIPU = 'zhipu',
}

export interface LLMMessage {
  role: 'system' | 'user' | 'assistant';
  content: string;
  timestamp?: number;
}

export interface LLMRequest {
  messages: LLMMessage[];
  temperature?: number;
  maxTokens?: number;
  stream?: boolean;
  tools?: any[];
}

export interface LLMResponse {
  content: string;
  finishReason: string;
  usage: {
    promptTokens: number;
    completionTokens: number;
    totalTokens: number;
  };
  model: string;
}

// LLM适配器基类
export abstract class LLMAdapter {
  abstract chat(request: LLMRequest): Promise<LLMResponse>;
  abstract chatStream(request: LLMRequest): AsyncIterable<string>;
  abstract countTokens(text: string): number;
}

// OpenAI适配器
export class OpenAIAdapter extends LLMAdapter {
  private client: OpenAI;

  constructor(apiKey: string, baseURL?: string) {
    super();
    this.client = new OpenAI({
      apiKey,
      baseURL: baseURL || 'https://api.openai.com/v1',
    });
  }

  async chat(request: LLMRequest): Promise<LLMResponse> {
    const response = await this.client.chat.completions.create({
      model: 'gpt-4o-mini',
      messages: request.messages,
      temperature: request.temperature ?? 0.7,
      max_tokens: request.maxTokens ?? 2000,
    });

    return {
      content: response.choices[0]?.message?.content ?? '',
      finishReason: response.choices[0]?.finish_reason ?? 'unknown',
      usage: {
        promptTokens: response.usage?.prompt_tokens ?? 0,
        completionTokens: response.usage?.completion_tokens ?? 0,
        totalTokens: response.usage?.total_tokens ?? 0,
      },
      model: response.model,
    };
  }

  async *chatStream(request: LLMRequest): AsyncIterable<string> {
    const stream = await this.client.chat.completions.create({
      model: 'gpt-4o-mini',
      messages: request.messages,
      temperature: request.temperature ?? 0.7,
      max_tokens: request.maxTokens ?? 2000,
      stream: true,
    });

    for await (const chunk of stream) {
      const content = chunk.choices[0]?.delta?.content;
      if (content) {
        yield content;
      }
    }
  }

  countTokens(text: string): number {
    // 简化的token计数，实际使用tiktoken
    return Math.ceil(text.length / 4);
  }
}

// 通义千问适配器
export class QwenAdapter extends LLMAdapter {
  private client: OpenAI;

  constructor(apiKey: string) {
    super();
    this.client = new OpenAI({
      apiKey,
      baseURL: 'https://dashscope.aliyuncs.com/compatible-mode/v1',
    });
  }

  async chat(request: LLMRequest): Promise<LLMResponse> {
    const response = await this.client.chat.completions.create({
      model: 'qwen-plus',
      messages: request.messages,
      temperature: request.temperature ?? 0.7,
      max_tokens: request.maxTokens ?? 2000,
    });

    return {
      content: response.choices[0]?.message?.content ?? '',
      finishReason: response.choices[0]?.finish_reason ?? 'unknown',
      usage: {
        promptTokens: response.usage?.prompt_tokens ?? 0,
        completionTokens: response.usage?.completion_tokens ?? 0,
        totalTokens: response.usage?.total_tokens ?? 0,
      },
      model: response.model,
    };
  }

  async *chatStream(request: LLMRequest): AsyncIterable<string> {
    const stream = await this.client.chat.completions.create({
      model: 'qwen-plus',
      messages: request.messages,
      temperature: request.temperature ?? 0.7,
      max_tokens: request.maxTokens ?? 2000,
      stream: true,
    });

    for await (const chunk of stream) {
      const content = chunk.choices[0]?.delta?.content;
      if (content) {
        yield content;
      }
    }
  }

  countTokens(text: string): number {
    return Math.ceil(text.length / 2);
  }
}

// LLM服务工厂
export class LLMServiceFactory {
  private static adapters: Map<LLMProvider, LLMAdapter> = new Map();

  static register(provider: LLMProvider, adapter: LLMAdapter): void {
    this.adapters.set(provider, adapter);
  }

  static get(provider: LLMProvider): LLMAdapter {
    const adapter = this.adapters.get(provider);
    if (!adapter) {
      throw new Error(`No adapter found for provider: ${provider}`);
    }
    return adapter;
  }

  static async chat(
    provider: LLMProvider,
    request: LLMRequest
  ): Promise<LLMResponse> {
    const adapter = this.get(provider);
    return adapter.chat(request);
  }

  static async *chatStream(
    provider: LLMProvider,
    request: LLMRequest
  ): AsyncIterable<string> {
    const adapter = this.get(provider);
    yield* adapter.chatStream(request);
  }
}
```

##### 3.2.2 智能模型路由

```typescript
// src/ai/llm/router.ts
export interface ModelCapability {
  maxTokens: number;
  supportsStreaming: boolean;
  supportsVision: boolean;
  supportsFunctionCalling: boolean;
  costPer1kTokens: number;
  averageLatency: number;
}

export interface ModelConfig {
  provider: LLMProvider;
  model: string;
  capability: ModelCapability;
}

export const MODEL_CAPABILITIES: Record<string, ModelConfig> = {
  'gpt-4o-mini': {
    provider: LLMProvider.OPENAI,
    model: 'gpt-4o-mini',
    capability: {
      maxTokens: 128000,
      supportsStreaming: true,
      supportsVision: true,
      supportsFunctionCalling: true,
      costPer1kTokens: 0.00015,
      averageLatency: 500,
    },
  },
  'gpt-4o': {
    provider: LLMProvider.OPENAI,
    model: 'gpt-4o',
    capability: {
      maxTokens: 128000,
      supportsStreaming: true,
      supportsVision: true,
      supportsFunctionCalling: true,
      costPer1kTokens: 0.0025,
      averageLatency: 800,
    },
  },
  'qwen-plus': {
    provider: LLMProvider.QWEN,
    model: 'qwen-plus',
    capability: {
      maxTokens: 128000,
      supportsStreaming: true,
      supportsVision: false,
      supportsFunctionCalling: true,
      costPer1kTokens: 0.0004,
      averageLatency: 600,
    },
  },
  'qwen-max': {
    provider: LLMProvider.QWEN,
    model: 'qwen-max',
    capability: {
      maxTokens: 128000,
      supportsStreaming: true,
      supportsVision: false,
      supportsFunctionCalling: true,
      costPer1kTokens: 0.0012,
      averageLatency: 1000,
    },
  },
};

export class ModelRouter {
  // 根据任务类型选择模型
  static selectModel(
    taskType: 'chat' | 'analysis' | 'creative' | 'urgent',
    requireVision?: boolean
  ): string {
    switch (taskType) {
      case 'chat':
        // 日常对话，使用高性价比模型
        return 'qwen-plus';

      case 'analysis':
        // 分析任务，需要更强的推理能力
        return 'gpt-4o';

      case 'creative':
        // 创意任务，需要更强的创造力
        return 'gpt-4o';

      case 'urgent':
        // 紧急任务，使用最低延迟模型
        const modelsByLatency = Object.entries(MODEL_CAPABILITIES)
          .filter(([_, config]) => !requireVision || config.capability.supportsVision)
          .sort((a, b) => a[1].capability.averageLatency - b[1].capability.averageLatency);
        return modelsByLatency[0]?.[0] ?? 'qwen-plus';

      default:
        return 'qwen-plus';
    }
  }

  // 根据token数量选择模型
  static selectModelByTokenCount(estimatedTokens: number): string {
    for (const [modelName, config] of Object.entries(MODEL_CAPABILITIES)) {
      if (config.capability.maxTokens >= estimatedTokens) {
        // 找到第一个满足需求的模型
        return modelName;
      }
    }
    // 如果都不满足，返回上下文最大的模型
    const maxContextModel = Object.entries(MODEL_CAPABILITIES)
      .sort((a, b) => b[1].capability.maxTokens - a[1].capability.maxTokens)[0];
    return maxContextModel[0];
  }

  // 成本优化路由
  static selectModelByBudget(
    maxCostPerRequest: number,
    estimatedTokens: number
  ): string {
    const affordableModels = Object.entries(MODEL_CAPABILITIES)
      .filter(([_, config]) => {
        const estimatedCost = (estimatedTokens / 1000) * config.capability.costPer1kTokens;
        return estimatedCost <= maxCostPerRequest;
      })
      .sort((a, b) => a[1].capability.costPer1kTokens - b[1].capability.costPer1kTokens);

    return affordableModels[0]?.[0] ?? 'qwen-plus';
  }
}
```

#### 3.3 LangChain集成

##### 3.3.1 Chain定义

```typescript
// src/ai/langchain/chains.ts
import { ChatOpenAI } from '@langchain/openai';
import { PromptTemplate } from '@langchain/core/prompts';
import { StringOutputParser } from '@langchain/core/output_parsers';
import { RunnableSequence } from '@langchain/core/runnables';

// 健康咨询Chain
export class HealthConsultationChain {
  private chain: RunnableSequence;

  constructor(llm: ChatOpenAI) {
    const prompt = PromptTemplate.fromTemplate(`
你是一位专业的健康顾问，为HaiLan Pro平台提供咨询服务。

用户画像：
- 年龄：{age}
- 性别：{gender}
- 健康关注点：{healthConcerns}

用户问题：{question}

请提供专业、贴心的健康建议，注意：
1. 建议要基于科学依据
2. 如涉及严重健康问题，建议及时就医
3. 保持友好、专业的语气
4. 回答要简洁明了，不超过200字

健康建议：
`);

    this.chain = RunnableSequence.from([prompt, llm, new StringOutputParser()]);
  }

  async consult(input: {
    age: number;
    gender: string;
    healthConcerns: string;
    question: string;
  }): Promise<string> {
    return await this.chain.invoke(input);
  }
}

// 情感陪伴Chain
export class EmotionalCompanionChain {
  private chain: RunnableSequence;

  constructor(llm: ChatOpenAI) {
    const prompt = PromptTemplate.fromTemplate(`
你是HaiLan Pro的AI情感陪伴助手，名为"小蓝"。

对话历史：
{chatHistory}

当前情绪：{mood}
用户当前状态：{userState}

用户说：{userInput}

请以温暖、理解的方式回应，注意：
1. 表达共情和理解
2. 提供积极的支持
3. 适当引导话题
4. 回答自然、口语化
5. 长度控制在100字以内

小蓝的回应：
`);

    this.chain = RunnableSequence.from([prompt, llm, new StringOutputParser()]);
  }

  async chat(input: {
    chatHistory: string;
    mood: string;
    userState: string;
    userInput: string;
  }): Promise<string> {
    return await this.chain.invoke(input);
  }
}

// 产品推荐Chain
export class ProductRecommendationChain {
  private chain: RunnableSequence;

  constructor(llm: ChatOpenAI) {
    const prompt = PromptTemplate.fromTemplate(`
你是HaiLan Pro的智能购物顾问，根据用户需求推荐合适的产品。

用户画像：
- 年龄：{age}
- 性别：{gender}
- 预算范围：{budget}
- 已购买产品：{purchasedProducts}

用户需求：{requirement}

可选产品：
{productCatalog}

请推荐2-3款最合适的产品，格式如下：
1. [产品名称] - 推荐理由
2. [产品名称] - 推荐理由
...

推荐结果：
`);

    this.chain = RunnableSequence.from([prompt, llm, new StringOutputParser()]);
  }

  async recommend(input: {
    age: number;
    gender: string;
    budget: string;
    purchasedProducts: string;
    requirement: string;
    productCatalog: string;
  }): Promise<string> {
    return await this.chain.invoke(input);
  }
}
```

##### 3.3.2 Agent定义

```typescript
// src/ai/langchain/agents.ts
import { AgentExecutor, createReactAgent } from 'langchain/agents';
import { Tool } from '@langchain/core/tools';
import { ChatOpenAI } from '@langchain/openai';

// 工具：查询产品信息
class ProductQueryTool extends Tool {
  name = 'product_query';
  description = '查询产品信息，包括价格、库存、规格等。输入产品名称或ID。';

  async _input(input: string): Promise<string> {
    // 调用产品服务API
    const response = await fetch(`/api/products/search?q=${encodeURIComponent(input)}`);
    const data = await response.json();
    return JSON.stringify(data);
  }
}

// 工具：查询订单状态
class OrderStatusTool extends Tool {
  name = 'order_status';
  description = '查询订单状态和物流信息。输入订单号。';

  async _input(input: string): Promise<string> {
    const response = await fetch(`/api/orders/${input}/status`);
    const data = await response.json();
    return JSON.stringify(data);
  }
}

// 工具：查询健康档案
class HealthProfileTool extends Tool {
  name = 'health_profile';
  description = '查询用户的健康档案数据。';

  async _input(_: string): Promise<string> {
    const response = await fetch('/api/health/profile');
    const data = await response.json();
    return JSON.stringify(data);
  }
}

// 工具：记录用户反馈
class FeedbackTool extends Tool {
  name = 'record_feedback';
  description = '记录用户反馈或建议。输入反馈内容。';

  async _input(input: string): Promise<string> {
    const response = await fetch('/api/feedback', {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({ content: input }),
    });
    return '反馈已记录';
  }
}

// 客服Agent
export class CustomerServiceAgent {
  private executor: AgentExecutor;

  constructor(llm: ChatOpenAI) {
    const tools = [
      new ProductQueryTool(),
      new OrderStatusTool(),
      new HealthProfileTool(),
      new FeedbackTool(),
    ];

    const prompt = `
你是HaiLan Pro的智能客服助手，帮助用户解决各种问题。

可用工具：
{tools}

工具使用格式：
Action: 工具名称
Action Input: 工具输入

观察：工具返回结果
...（可以多次使用工具）

Thought: 我现在知道最终答案了
Final Answer: 对用户的最终回复

开始！

用户问题：{input}
Thought: {agent_scratchpad}
`;

    const agent = createReactAgent({ llm, tools, prompt });
    this.executor = AgentExecutor.fromAgentAndTools({
      agent,
      tools,
      verbose: true,
      maxIterations: 5,
    });
  }

  async assist(userInput: string): Promise<string> {
    const result = await this.executor.invoke({ input: userInput });
    return result.output as string;
  }
}
```

#### 3.4 RAG架构设计

##### 3.4.1 向量存储

```typescript
// src/ai/rag/vectorstore.ts
import { SupabaseVectorStore } from '@langchain/community/vectorstores/supabase';
import { OpenAIEmbeddings } from '@langchain/openai';
import { createClient } from '@supabase/supabase-js';

export interface DocumentChunk {
  id: string;
  content: string;
  metadata: {
    source: string;
    category: string;
    tags: string[];
    author?: string;
    createdAt: string;
  };
}

export class HealthKnowledgeVectorStore {
  private store: SupabaseVectorStore;
  private embeddings: OpenAIEmbeddings;

  constructor() {
    const supabase = createClient(
      process.env.VITE_SUPABASE_URL!,
      process.env.VITE_SUPABASE_ANON_KEY!
    );

    this.embeddings = new OpenAIEmbeddings({
      modelName: 'text-embedding-3-small',
    });

    this.store = await SupabaseVectorStore.initialize(
      this.embeddings,
      {
        client: supabase,
        tableName: 'health_knowledge_embeddings',
        queryName: 'match_health_knowledge',
      }
    );
  }

  // 添加文档
  async addDocuments(chunks: DocumentChunk[]): Promise<void> {
    const documents = chunks.map(chunk => ({
      pageContent: chunk.content,
      metadata: chunk.metadata,
    }));

    await this.store.addDocuments(documents);
  }

  // 相似度搜索
  async similaritySearch(
    query: string,
    k: number = 5,
    filter?: { category: string }
  ): Promise<DocumentChunk[]> {
    const results = await this.store.similaritySearchWithScore(query, k, {
      category: filter?.category,
    });

    return results.map(([doc, score]) => ({
      id: doc.metadata.id as string,
      content: doc.pageContent,
      metadata: doc.metadata,
    }));
  }

  // 混合搜索（向量+关键词）
  async hybridSearch(
    query: string,
    k: number = 5
  ): Promise<DocumentChunk[]> {
    // 向量搜索
    const vectorResults = await this.similaritySearch(query, k);

    // 关键词搜索（通过Supabase全文搜索）
    const keywordResults = await this.keywordSearch(query, k);

    // 合并和重排序
    return this.mergeAndRerank(vectorResults, keywordResults, k);
  }

  private async keywordSearch(
    query: string,
    k: number
  ): Promise<DocumentChunk[]> {
    // 实现关键词搜索
    return [];
  }

  private mergeAndRerank(
    vectorResults: DocumentChunk[],
    keywordResults: DocumentChunk[],
    k: number
  ): DocumentChunk[]> {
    // 实现合并和重排序逻辑
    return [...vectorResults].slice(0, k);
  }
}
```

##### 3.4.2 RAG管道

```typescript
// src/ai/rag/pipeline.ts
export class RAGPipeline {
  private vectorStore: HealthKnowledgeVectorStore;
  private llm: LLMAdapter;

  constructor() {
    this.vectorStore = new HealthKnowledgeVectorStore();
    this.llm = LLMServiceFactory.get(LLMProvider.QWEN);
  }

  // 查询增强生成
  async query(question: string, context?: {
    category?: string;
    userId?: string;
  }): Promise<string> {
    // 1. 检索相关知识
    const relevantDocs = await this.vectorStore.similaritySearch(
      question,
      5,
      { category: context?.category }
    );

    // 2. 构建上下文
    const contextText = relevantDocs
      .map((doc, i) => `[参考资料${i + 1}]\n${doc.content}`)
      .join('\n\n');

    // 3. 构建提示词
    const systemPrompt = `
你是HaiLan Pro的健康顾问，基于专业知识回答用户问题。

参考资料：
${contextText}

请基于以上参考资料回答用户问题。如果参考资料中没有相关信息，
请明确告知，不要编造信息。

回答要求：
1. 专业准确
2. 简洁明了
3. 引用具体的参考资料
4. 如涉及严重健康问题，建议就医
`;

    // 4. 调用LLM生成回答
    const response = await this.llm.chat({
      messages: [
        { role: 'system', content: systemPrompt },
        { role: 'user', content: question },
      ],
      temperature: 0.3,
      maxTokens: 1000,
    });

    // 5. 记录查询日志
    await this.logQuery({
      question,
      context: contextText,
      answer: response.content,
      sources: relevantDocs.map(d => d.id),
      userId: context?.userId,
    });

    return response.content;
  }

  // 流式响应
  async *queryStream(
    question: string,
    context?: { category?: string; userId?: string }
  ): AsyncIterable<string> {
    const relevantDocs = await this.vectorStore.similaritySearch(
      question,
      5,
      { category: context?.category }
    );

    const contextText = relevantDocs
      .map((doc, i) => `[参考资料${i + 1}]\n${doc.content}`)
      .join('\n\n');

    const systemPrompt = `
你是HaiLan Pro的健康顾问，基于专业知识回答用户问题。

参考资料：
${contextText}

请基于以上参考资料回答用户问题。
`;

    yield* this.llm.chatStream({
      messages: [
        { role: 'system', content: systemPrompt },
        { role: 'user', content: question },
      ],
      temperature: 0.3,
      maxTokens: 1000,
    });
  }

  private async logQuery(data: any): Promise<void> {
    // 记录到数据库
    await fetch('/api/ai/queries', {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify(data),
    });
  }
}
```

#### 3.5 提示词管理

##### 3.5.1 提示词模板

```typescript
// src/ai/prompts/templates.ts
export interface PromptTemplate {
  id: string;
  name: string;
  category: string;
  template: string;
  variables: string[];
  version: string;
}

export const PROMPT_TEMPLATES: Record<string, PromptTemplate> = {
  health_consultation: {
    id: 'health_consultation',
    name: '健康咨询',
    category: 'health',
    template: `你是一位专业的健康顾问，为HaiLan Pro平台提供咨询服务。

用户画像：
- 年龄：{age}
- 性别：{gender}
- 健康关注点：{healthConcerns}

用户问题：{question}

请提供专业、贴心的健康建议，注意：
1. 建议要基于科学依据
2. 如涉及严重健康问题，建议及时就医
3. 保持友好、专业的语气
4. 回答要简洁明了，不超过200字

健康建议：`,
    variables: ['age', 'gender', 'healthConcerns', 'question'],
    version: '1.0.0',
  },

  emotional_support: {
    id: 'emotional_support',
    name: '情感支持',
    category: 'emotional',
    template: `你是HaiLan Pro的AI情感陪伴助手"小蓝"。

对话历史：
{chatHistory}

用户当前情绪：{mood}

用户说：{userInput}

请以温暖、理解的方式回应，注意：
1. 表达共情和理解
2. 提供积极的支持
3. 适当引导话题
4. 回答自然、口语化
5. 长度控制在100字以内

小蓝的回应：`,
    variables: ['chatHistory', 'mood', 'userInput'],
    version: '1.0.0',
  },

  product_recommendation: {
    id: 'product_recommendation',
    name: '产品推荐',
    category: 'commerce',
    template: `你是HaiLan Pro的智能购物顾问。

用户画像：
- 年龄：{age}
- 性别：{gender}
- 预算：{budget}

用户需求：{requirement}

可选产品：
{productCatalog}

请推荐2-3款最合适的产品，说明推荐理由：
1. [产品名称] - 推荐理由
2. [产品名称] - 推荐理由
...`,
    variables: ['age', 'gender', 'budget', 'requirement', 'productCatalog'],
    version: '1.0.0',
  },

  privacy_mode: {
    id: 'privacy_mode',
    name: '隐私模式回应',
    category: 'privacy',
    template: `用户当前处于隐私保护模式，所有回答需要特别注意隐私保护。

用户问题：{question}

回答时请注意：
1. 不提及任何敏感词汇
2. 使用委婉、含蓄的表达
3. 避免可能引起他人注意的内容
4. 如涉及敏感话题，引导用户切换隐私模式

回应：`,
    variables: ['question'],
    version: '1.0.0',
  },
};

export class PromptManager {
  // 渲染提示词模板
  static render(templateId: string, variables: Record<string, any>): string {
    const template = PROMPT_TEMPLATES[templateId];
    if (!template) {
      throw new Error(`Template not found: ${templateId}`);
    }

    let result = template.template;

    for (const variable of template.variables) {
      const placeholder = `{${variable}}`;
      const value = variables[variable] ?? '';
      result = result.replace(new RegExp(placeholder, 'g'), value);
    }

    return result;
  }

  // A/B测试提示词
  static async testVariants(
    templateId: string,
    variables: Record<string, any>
  ): Promise<Array<{ variant: string; response: string }>> {
    // 获取提示词变体
    const variants = await this.getVariants(templateId);

    const results = await Promise.all(
      variants.map(async (variant) => {
        const rendered = this.render(variant.id, variables);
        const response = await LLMServiceFactory.chat(LLMProvider.QWEN, {
          messages: [{ role: 'user', content: rendered }],
        });
        return {
          variant: variant.name,
          response: response.content,
        };
      })
    );

    return results;
  }

  private static async getVariants(templateId: string): Promise<PromptTemplate[]> {
    // 获取提示词变体（从数据库）
    return [PROMPT_TEMPLATES[templateId]];
  }
}
```

#### 3.6 智能推荐系统

##### 3.6.1 推荐引擎

```typescript
// src/ai/recommendation/engine.ts
export interface UserProfile {
  userId: string;
  age: number;
  gender: string;
  interests: string[];
  purchaseHistory: string[];
  browsingHistory: string[];
  healthProfile?: {
    concerns: string[];
    goals: string[];
  };
}

export interface Product {
  id: string;
  name: string;
  category: string;
  price: number;
  tags: string[];
  description: string;
  healthBenefits?: string[];
}

export class RecommendationEngine {
  // 基于内容的推荐
  async contentBasedRecommend(
    user: UserProfile,
    products: Product[]
  ): Promise<Product[]> {
    const scores = products.map(product => ({
      product,
      score: this.calculateContentScore(user, product),
    }));

    return scores
      .sort((a, b) => b.score - a.score)
      .slice(0, 10)
      .map(s => s.product);
  }

  // 协同过滤推荐
  async collaborativeRecommend(
    user: UserProfile,
    topK: number = 10
  ): Promise<Product[]> {
    // 找到相似用户
    const similarUsers = await this.findSimilarUsers(user, 20);

    // 聚合相似用户喜欢的产品
    const productScores = new Map<string, number>();

    for (const similarUser of similarUsers) {
      for (const productId of similarUser.purchaseHistory) {
        const score = productScores.get(productId) ?? 0;
        productScores.set(productId, score + similarUser.similarity);
      }
    }

    // 排除用户已购买的产品
    for (const purchased of user.purchaseHistory) {
      productScores.delete(purchased);
    }

    // 返回top-k产品
    const sorted = Array.from(productScores.entries())
      .sort((a, b) => b[1] - a[1])
      .slice(0, topK)
      .map(([productId]) => productId);

    return this.getProductsByIds(sorted);
  }

  // 混合推荐
  async hybridRecommend(
    user: UserProfile,
    products: Product[]
  ): Promise<Product[]> {
    // 内容推荐
    const contentRecs = await this.contentBasedRecommend(user, products);

    // 协同过滤推荐
    const collabRecs = await this.collaborativeRecommend(user, 10);

    // AI辅助推荐
    const aiRecs = await this.aiAssistRecommend(user, products);

    // 融合三种推荐结果
    return this.mergeRecommendations(contentRecs, collabRecs, aiRecs);
  }

  private calculateContentScore(user: UserProfile, product: Product): number {
    let score = 0;

    // 兴趣匹配
    for (const interest of user.interests) {
      if (product.tags.includes(interest)) {
        score += 0.3;
      }
    }

    // 浏览历史匹配
    for (const browsed of user.browsingHistory) {
      if (product.category === browsed) {
        score += 0.2;
      }
    }

    // 健康目标匹配
    if (user.healthProfile) {
      for (const concern of user.healthProfile.concerns) {
        if (product.healthBenefits?.includes(concern)) {
          score += 0.4;
        }
      }
    }

    return score;
  }

  private async findSimilarUsers(
    user: UserProfile,
    limit: number
  ): Promise<Array<{ userId: string; similarity: number; purchaseHistory: string[] }>> {
    // 计算用户相似度（余弦相似度）
    return [];
  }

  private async aiAssistRecommend(
    user: UserProfile,
    products: Product
  ): Promise<Product[]> {
    const prompt = PromptManager.render('product_recommendation', {
      age: user.age,
      gender: user.gender,
      budget: 'medium',
      requirement: user.interests.join(', '),
      productCatalog: products.slice(0, 20).map(p =>
        `${p.name} - ${p.description}`
      ).join('\n'),
    });

    const response = await LLMServiceFactory.chat(LLMProvider.QWEN, {
      messages: [{ role: 'user', content: prompt }],
    });

    // 解析AI推荐的productIds
    return this.parseAIRecommendations(response.content, products);
  }

  private mergeRecommendations(
    contentRecs: Product[],
    collabRecs: Product[],
    aiRecs: Product[]
  ): Product[] {
    const merged = new Map<string, { product: Product; score: number }>();

    // 内容推荐权重: 0.3
    contentRecs.forEach((product, i) => {
      const score = (10 - i) * 0.3;
      merged.set(product.id, { product, score: (merged.get(product.id)?.score ?? 0) + score });
    });

    // 协同过滤权重: 0.4
    collabRecs.forEach((product, i) => {
      const score = (10 - i) * 0.4;
      merged.set(product.id, { product, score: (merged.get(product.id)?.score ?? 0) + score });
    });

    // AI推荐权重: 0.3
    aiRecs.forEach((product, i) => {
      const score = (10 - i) * 0.3;
      merged.set(product.id, { product, score: (merged.get(product.id)?.score ?? 0) + score });
    });

    return Array.from(merged.values())
      .sort((a, b) => b.score - a.score)
      .slice(0, 10)
      .map(v => v.product);
  }

  private async getProductsByIds(ids: string[]): Promise<Product[]> {
    // 从数据库获取产品
    return [];
  }

  private parseAIRecommendations(text: string, products: Product[]): Product[] {
    // 解析AI返回的推荐文本，匹配到产品
    return [];
  }
}
```

#### 3.7 AI安全与合规

##### 3.7.1 内容过滤

```typescript
// src/ai/safety/moderation.ts
export interface ModerationResult {
  safe: boolean;
  categories: {
    hate: boolean;
    violence: boolean;
    sexual: boolean;
    selfHarm: boolean;
    pii: boolean;
  };
  confidence: number;
}

export class ContentModerator {
  // 输入过滤
  async moderateInput(input: string): Promise<ModerationResult> {
    // 调用OpenAI Moderation API
    const response = await fetch('https://api.openai.com/v1/moderations', {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
        'Authorization': `Bearer ${process.env.OPENAI_API_KEY}`,
      },
      body: JSON.stringify({ input }),
    });

    const data = await response.json();
    const result = data.results[0];

    return {
      safe: !result.flagged,
      categories: {
        hate: result.categories.hate,
        violence: result.categories.violence,
        sexual: result.categories.sexual,
        selfHarm: result.categories['self-harm'],
        pii: false, // 需要单独检测
      },
      confidence: result.category_scores.hate || 0,
    };
  }

  // PII检测与脱敏
  async detectAndMaskPII(text: string): Promise<{
    masked: string;
    detectedPII: Array<{ type: string; value: string }>;
  }> {
    const detectedPII: Array<{ type: string; value: string }> = [];
    let masked = text;

    // 手机号检测
    const phoneRegex = /1[3-9]\d{9}/g;
    const phones = text.match(phoneRegex) || [];
    phones.forEach(phone => {
      detectedPII.push({ type: 'phone', value: phone });
      masked = masked.replace(phone, '***');
    });

    // 邮箱检测
    const emailRegex = /\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b/g;
    const emails = text.match(emailRegex) || [];
    emails.forEach(email => {
      detectedPII.push({ type: 'email', value: email });
      masked = masked.replace(email, '***@***.***');
    });

    // 身份证号检测
    const idCardRegex = /\d{17}[\dXx]/g;
    const idCards = text.match(idCardRegex) || [];
    idCards.forEach(idCard => {
      detectedPII.push({ type: 'idCard', value: idCard });
      masked = masked.replace(idCard, '******************');
    });

    return { masked, detectedPII };
  }

  // 敏感词过滤
  async filterSensitiveWords(text: string): Promise<{
    filtered: string;
    detectedWords: string[];
  }> {
    const sensitiveWords = await this.loadSensitiveWords();
    const detectedWords: string[] = [];
    let filtered = text;

    for (const word of sensitiveWords) {
      if (text.includes(word)) {
        detectedWords.push(word);
        filtered = filtered.replace(new RegExp(word, 'g'), '*'.repeat(word.length));
      }
    }

    return { filtered, detectedWords };
  }

  private async loadSensitiveWords(): Promise<string[]> {
    // 从数据库或配置文件加载敏感词库
    return [];
  }
}
```

##### 3.7.2 对话加密存储

```typescript
// src/ai/security/encryption.ts
import { encryptData, decryptData } from '@/lib/crypto';

export interface ChatMessage {
  role: 'user' | 'assistant';
  content: string;
  timestamp: number;
}

export class SecureChatStorage {
  // 加密存储对话
  async saveChat(
    userId: string,
    sessionId: string,
    messages: ChatMessage[]
  ): Promise<void> {
    // 序列化
    const json = JSON.stringify(messages);

    // 加密
    const encrypted = await encryptData(json);

    // 存储到数据库
    await fetch('/api/ai/chats', {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({
        userId,
        sessionId,
        encryptedData: encrypted.data,
        iv: encrypted.iv,
      }),
    });
  }

  // 解密读取对话
  async loadChat(
    userId: string,
    sessionId: string
  ): Promise<ChatMessage[]> {
    const response = await fetch(`/api/ai/chats/${userId}/${sessionId}`);
    const data = await response.json();

    // 解密
    const decrypted = await decryptData({
      data: data.encryptedData,
      iv: data.iv,
    });

    return JSON.parse(decrypted);
  }

  // 自动过期删除
  async autoDeleteExpiredChats(): Promise<void> {
    const daysToKeep = 30; // 保留30天
    const cutoffDate = new Date(Date.now() - daysToKeep * 24 * 60 * 60 * 1000);

    await fetch(`/api/ai/chats/cleanup?before=${cutoffDate.toISOString()}`, {
      method: 'DELETE',
    });
  }
}
```

### 4. 部署配置

#### 4.1 环境变量

```bash
# .env
# OpenAI
OPENAI_API_KEY=sk-xxx
OPENAI_BASE_URL=https://api.openai.com/v1

# 通义千问
QWEN_API_KEY=sk-xxx
QWEN_BASE_URL=https://dashscope.aliyuncs.com/compatible-mode/v1

# DeepSeek
DEEPSEEK_API_KEY=sk-xxx

# Supabase (用于向量存储)
SUPABASE_URL=https://xxx.supabase.co
SUPABASE_ANON_KEY=eyJxxx
SUPABASE_SERVICE_ROLE_KEY=eyJxxx

# AI服务配置
AI_DEFAULT_PROVIDER=qwen
AI_MAX_TOKENS=2000
AI_TEMPERATURE=0.7
AI_STREAM_ENABLED=true

# RAG配置
RAG_TOP_K=5
RAG_SIMILARITY_THRESHOLD=0.7
```

#### 4.2 API端点

```typescript
// src/ai/api/routes.ts
import { Router } from 'express';

const router = Router();

// AI健康咨询
router.post('/consult', async (req, res) => {
  const { question, userProfile } = req.body;

  const moderationResult = await moderator.moderateInput(question);
  if (!moderationResult.safe) {
    return res.status(400).json({ error: 'Content violation detected' });
  }

  const ragPipeline = new RAGPipeline();
  const answer = await ragPipeline.query(question, {
    category: 'health',
    userId: userProfile.userId,
  });

  res.json({ answer });
});

// 流式AI对话
router.post('/chat/stream', async (req, res) => {
  const { messages } = req.body;

  res.setHeader('Content-Type', 'text/event-stream');
  res.setHeader('Cache-Control', 'no-cache');

  const llm = LLMServiceFactory.get(LLMProvider.QWEN);
  const stream = llm.chatStream({ messages });

  for await (const chunk of stream) {
    res.write(`data: ${JSON.stringify({ content: chunk })}\n\n`);
  }

  res.write('data: [DONE]\n\n');
  res.end();
});

// 产品推荐
router.post('/recommend', async (req, res) => {
  const { userProfile, products } = req.body;

  const engine = new RecommendationEngine();
  const recommendations = await engine.hybridRecommend(userProfile, products);

  res.json({ recommendations });
});

export default router;
```

### 5. 监控与优化

#### 5.1 AI服务指标

```typescript
// src/ai/monitoring/metrics.ts
export interface AIMetrics {
  // 调用指标
  totalRequests: number;
  successfulRequests: number;
  failedRequests: number;
  averageResponseTime: number;

  // Token使用
  totalPromptTokens: number;
  totalCompletionTokens: number;
  totalTokens: number;

  // 成本
  estimatedCost: number;

  // 质量
  averageRating: number;
  thumbsUpRatio: number;
}

export class AIMetricsCollector {
  private metrics: Partial<AIMetrics> = {};

  recordRequest(responseTime: number, success: boolean): void {
    this.metrics.totalRequests = (this.metrics.totalRequests ?? 0) + 1;
    if (success) {
      this.metrics.successfulRequests = (this.metrics.successfulRequests ?? 0) + 1;
    } else {
      this.metrics.failedRequests = (this.metrics.failedRequests ?? 0) + 1;
    }

    // 更新平均响应时间
    const total = this.metrics.totalRequests;
    const currentAvg = this.metrics.averageResponseTime ?? 0;
    this.metrics.averageResponseTime =
      (currentAvg * (total - 1) + responseTime) / total;
  }

  recordTokenUsage(promptTokens: number, completionTokens: number): void {
    this.metrics.totalPromptTokens =
      (this.metrics.totalPromptTokens ?? 0) + promptTokens;
    this.metrics.totalCompletionTokens =
      (this.metrics.totalCompletionTokens ?? 0) + completionTokens;
    this.metrics.totalTokens =
      (this.metrics.totalTokens ?? 0) + promptTokens + completionTokens;
  }

  calculateCost(provider: LLMProvider): number {
    const rates = {
      [LLMProvider.OPENAI]: { prompt: 0.00015, completion: 0.0006 },
      [LLMProvider.QWEN]: { prompt: 0.0001, completion: 0.0002 },
    };

    const rate = rates[provider];
    const cost =
      (this.metrics.totalPromptTokens ?? 0) * rate.prompt +
      (this.metrics.totalCompletionTokens ?? 0) * rate.completion;

    this.metrics.estimatedCost = cost;
    return cost;
  }

  getMetrics(): AIMetrics {
    return {
      totalRequests: this.metrics.totalRequests ?? 0,
      successfulRequests: this.metrics.successfulRequests ?? 0,
      failedRequests: this.metrics.failedRequests ?? 0,
      averageResponseTime: this.metrics.averageResponseTime ?? 0,
      totalPromptTokens: this.metrics.totalPromptTokens ?? 0,
      totalCompletionTokens: this.metrics.totalCompletionTokens ?? 0,
      totalTokens: this.metrics.totalTokens ?? 0,
      estimatedCost: this.metrics.estimatedCost ?? 0,
      averageRating: this.metrics.averageRating ?? 0,
      thumbsUpRatio: this.metrics.thumbsUpRatio ?? 0,
    };
  }
}
```

---

> 「***YanYuCloudCube***」
> 「***<admin@0379.email>***」
> 「***Words Initiate Quadrants, Language Serves as Core for the Future***」
> 「***All things converge in the cloud pivot; Deep stacks ignite a new era of intelligence***」
