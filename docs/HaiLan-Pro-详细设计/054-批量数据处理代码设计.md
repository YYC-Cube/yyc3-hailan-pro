---
@file: 054-批量数据处理代码设计.md
@description: HaiLan Pro 大批量数据导入、导出、处理的代码优化方案，保障性能与稳定性
@author: YanYuCloudCube Team
@version: v1.0.0
@created: 2026-01-26
@updated: 2026-01-26
@status: published
@tags: [HaiLan-Pro-详细设计],[]
---

> ***YanYuCloudCube***
> **标语**：言启象限 | 语枢未来
> ***Words Initiate Quadrants, Language Serves as Core for the Future***
> **标语**：万象归元于云枢 | 深栈智启新纪元
> ***All things converge in the cloud pivot; Deep stacks ignite a new era of intelligence***

---

# 054 批量数据处理代码设计

## 概述

本文档详细描述HaiLan Pro-HaiLan-Pro-详细设计-批量数据处理代码设计相关内容，确保项目按照YYC³标准规范进行开发和实施。

## 核心内容

### 1. 背景与目标

#### 1.1 项目背景
HaiLan Pro (海蓝) 是新一代高端、私密、智能的情趣健康生活管理平台。项目基于「五高五标五化」理念，通过 PWA 技术结合 AI 智能辅助与物联网，为用户提供从生理健康到心理愉悦的全方位解决方案。

#### 1.2 项目愿景
打造极致隐私、智能陪伴、品质合规、全场景覆盖的情趣健康生活管理平台，为用户提供安全、专业、高端的健康生活体验。

#### 1.3 核心价值主张
- **极致隐私**：双重加密、隐私浏览模式及伪装发货机制
- **智能陪伴**：基于 LLM 的 AI 情感与生理健康顾问
- **品质合规**：医疗级标准商品，高端"海蓝蓝"视觉调性
- **全场景覆盖**：PWA 端支持离线浏览、桌面安装及无缝推送

#### 1.4 文档目标
- 规范批量数据处理代码设计相关的业务标准与技术落地要求
- 为项目相关人员提供清晰的参考依据
- 保障相关模块开发、实施、运维的一致性与规范性

### 2. 设计原则

#### 2.1 五高原则
- **高可用性**：确保系统7x24小时稳定运行，支持PWA离线能力
- **高性能**：优化响应时间和处理能力，支持高并发访问
- **高安全性**：保护用户数据和隐私安全，双重加密机制
- **高扩展性**：支持业务快速扩展，微服务架构设计
- **高可维护性**：便于后续维护和升级，模块化设计

#### 2.2 五标体系
- **标准化**：统一的技术和流程标准
- **规范化**：严格的开发和管理规范
- **自动化**：提高开发效率和质量，CI/CD自动化
- **智能化**：利用AI技术提升能力，LLM智能顾问
- **可视化**：直观的监控和管理界面

#### 2.3 五化架构
- **流程化**：标准化的开发流程
- **文档化**：完善的文档体系
- **工具化**：高效的开发工具链
- **数字化**：数据驱动的决策
- **生态化**：开放的生态系统

### 3. 批量数据处理代码设计

#### 3.1 批量导入实现

##### 3.1.1 Excel文件解析

```typescript
// src/modules/batch/services/import.service.ts
import { Injectable } from '@nestjs/common';
import * as XLSX from 'xlsx';
import { Validator } from 'class-validator';
import { plainToClass } from 'class-transformer';

/**
 * 导入结果
 */
interface ImportResult {
  /** 成功数量 */
  success: number;
  /** 失败数量 */
  failed: number;
  /** 错误详情 */
  errors: ImportError[];
  /** 总耗时(ms) */
  duration: number;
}

/**
 * 导入错误详情
 */
interface ImportError {
  /** 行号 */
  row: number;
  /** 错误消息 */
  message: string;
  /** 原始数据 */
  data: any;
}

/**
 * 批量导入配置
 */
interface ImportConfig<T> {
  /** 数据类型（用于校验） */
  type: new () => T;
  /** 是否跳过空行 */
  skipEmpty?: boolean;
  /** 批次大小（分批处理） */
  batchSize?: number;
  /** 数据转换钩子 */
  transform?: (raw: any) => Promise<any>;
  /** 数据保存钩子 */
  save?: (data: T[], batch: number) => Promise<void>;
}

/**
 * 批量导入服务
 */
@Injectable()
export class ImportService {
  private readonly validator = new Validator();

  /**
   * 从Excel文件导入数据
   */
  async importFromExcel<T>(
    file: Express.Multer.File,
    config: ImportConfig<T>
  ): Promise<ImportResult> {
    const startTime = Date.now();
    const result: ImportResult = {
      success: 0,
      failed: 0,
      errors: [],
      duration: 0
    };

    try {
      // 1. 解析Excel文件
      const workbook = XLSX.read(file.buffer, { type: 'buffer' });
      const sheetName = workbook.SheetNames[0];
      const worksheet = workbook.Sheets[sheetName];

      // 2. 转换为JSON
      const rawData = XLSX.utils.sheet_to_json(worksheet, {
        defval: null,
        raw: false
      });

      // 3. 分批处理数据
      const batchSize = config.batchSize || 100;
      for (let i = 0; i < rawData.length; i += batchSize) {
        const batch = rawData.slice(i, i + batchSize);
        await this.processBatch(batch, i / batchSize + 1, config, result);
      }

    } catch (error) {
      result.errors.push({
        row: 0,
        message: `文件解析失败: ${error.message}`,
        data: null
      });
    }

    result.duration = Date.now() - startTime;
    return result;
  }

  /**
   * 处理一批数据
   * @private
   */
  private async processBatch<T>(
    batch: any[],
    batchNumber: number,
    config: ImportConfig<T>,
    result: ImportResult
  ): Promise<void> {
    const validData: T[] = [];

    for (let i = 0; i < batch.length; i++) {
      const row = i + 1;
      const item = batch[i];

      try {
        // 跳过空行
        if (config.skipEmpty && this.isEmpty(item)) {
          continue;
        }

        // 数据转换
        let transformed = item;
        if (config.transform) {
          transformed = await config.transform(item);
        }

        // 类型转换和校验
        const entity = plainToClass(config.type, transformed);
        const errors = await this.validator.validate(entity);

        if (errors.length > 0) {
          result.errors.push({
            row: row + (batchNumber - 1) * (config.batchSize || 100),
            message: this.formatValidationErrors(errors),
            data: item
          });
          result.failed++;
        } else {
          validData.push(entity);
          result.success++;
        }

      } catch (error) {
        result.errors.push({
          row: row,
          message: error.message,
          data: item
        });
        result.failed++;
      }
    }

    // 保存有效数据
    if (validData.length > 0 && config.save) {
      await config.save(validData, batchNumber);
    }
  }

  /**
   * 检查数据是否为空
   * @private
   */
  private isEmpty(data: any): boolean {
    return Object.values(data).every(
      value => value === null || value === undefined || value === ''
    );
  }

  /**
   * 格式化校验错误
   * @private
   */
  private formatValidationErrors(errors: any[]): string {
    return errors
      .map(err => Object.values(err.constraints).join(', '))
      .join('; ');
  }

  /**
   * 生成导入模板
   */
  generateTemplate<T>(type: new () => T): Buffer {
    const entity = new type();
    const columns = Object.keys(entity);

    const template = columns.map(col => ({
      [col]: `示例_${col}`
    }));

    const worksheet = XLSX.utils.json_to_sheet(template);
    const workbook = XLSX.utils.book_new();
    XLSX.utils.book_append_sheet(workbook, worksheet, '数据导入');

    return XLSX.write(workbook, { type: 'buffer', bookType: 'xlsx' });
  }
}
```

##### 3.1.2 CSV文件解析

```typescript
// src/modules/batch/services/csv-import.service.ts
import { Injectable } from '@nestjs/common';
import * as csv from 'csv-parser';
import { Readable } from 'stream';

/**
 * CSV导入服务
 */
@Injectable()
export class CsvImportService {
  /**
   * 解析CSV文件
   */
  async parseCsv(file: Express.Multer.File): Promise<any[]> {
    return new Promise((resolve, reject) => {
      const results: any[] = [];
      const stream = Readable.from(file.buffer);

      stream
        .pipe(csv())
        .on('data', (data) => results.push(data))
        .on('end', () => resolve(results))
        .on('error', (error) => reject(error));
    });
  }

  /**
   * 解析CSV流（大文件处理）
   */
  async parseCsvStream(
    file: Express.Multer.File,
    onRow: (row: any, index: number) => Promise<void>
  ): Promise<{ processed: number; failed: number }> {
    return new Promise((resolve, reject) => {
      let processed = 0;
      let failed = 0;
      let index = 0;

      const stream = Readable.from(file.buffer);

      stream
        .pipe(csv())
        .on('data', async (row) => {
          try {
            await onRow(row, index++);
            processed++;
          } catch (error) {
            failed++;
            console.error(`处理第${index}行失败:`, error);
          }
        })
        .on('end', () => resolve({ processed, failed }))
        .on('error', (error) => reject(error));
    });
  }
}
```

#### 3.2 批量导出实现

```typescript
// src/modules/batch/services/export.service.ts
import { Injectable } from '@nestjs/common';
import * as XLSX from 'xlsx';
import { Stream } from 'stream';
import * as csv from 'csv-writer';

/**
 * 导出配置
 */
interface ExportConfig {
  /** 文件名 */
  filename: string;
  /** 格式 */
  format: 'xlsx' | 'csv';
  /** 表头映射 */
  headers?: Record<string, string>;
  /** 数据转换函数 */
  transform?: (data: any) => any;
}

/**
 * 批量导出服务
 */
@Injectable()
export class ExportService {
  /**
   * 导出为Excel
   */
  async exportToExcel(
    data: any[],
    config: ExportConfig
  ): Promise<Buffer> {
    // 转换数据
    const transformed = config.transform
      ? data.map(config.transform)
      : data;

    // 应用表头映射
    const finalData = this.applyHeaders(transformed, config.headers);

    // 创建工作簿
    const worksheet = XLSX.utils.json_to_sheet(finalData);
    const workbook = XLSX.utils.book_new();
    XLSX.utils.book_append_sheet(workbook, worksheet, '数据');

    // 生成Buffer
    return XLSX.write(workbook, { type: 'buffer', bookType: 'xlsx' });
  }

  /**
   * 导出为CSV
   */
  async exportToCsv(
    data: any[],
    config: ExportConfig
  ): Promise<Buffer> {
    const transformed = config.transform
      ? data.map(config.transform)
      : data;

    const finalData = this.applyHeaders(transformed, config.headers);

    // 转换为CSV格式
    const csvData = finalData.map(row =>
      Object.values(row).map(v => `"${v}"`).join(',')
    );

    const headers = Object.keys(finalData[0] || {}).join(',');
    const csvContent = [headers, ...csvData].join('\n');

    return Buffer.from(csvContent, 'utf-8');
  }

  /**
   * 流式导出（大数据量）
   */
  exportAsStream(config: ExportConfig): Stream {
    const stream = new Stream.PassThrough();

    // TODO: 实现流式导出逻辑
    // 使用游标查询数据库，逐批写入流

    return stream;
  }

  /**
   * 应用表头映射
   * @private
   */
  private applyHeaders(
    data: any[],
    headers?: Record<string, string>
  ): any[] {
    if (!headers) return data;

    return data.map(item => {
      const mapped: any = {};
      for (const [key, value] of Object.entries(item)) {
        const newKey = headers[key] || key;
        mapped[newKey] = value;
      }
      return mapped;
    });
  }
}
```

#### 3.3 大数据处理策略

##### 3.3.1 分批查询与处理

```typescript
// src/modules/batch/services/batch-processor.service.ts
import { Injectable } from '@nestjs/common';
import { Repository } from 'typeorm';

/**
 * 批量处理配置
 */
interface BatchProcessConfig {
  /** 每批处理数量 */
  batchSize?: number;
  /** 并发批次数 */
  concurrency?: number;
  /** 进度回调 */
  onProgress?: (current: number, total: number) => void;
}

/**
 * 批量处理器服务
 */
@Injectable()
export class BatchProcessorService {
  /**
   * 分批查询并处理
   */
  async processBatch<T, R>(
    repository: Repository<T>,
    processFn: (items: T[]) => Promise<R[]>,
    config: BatchProcessConfig = {}
  ): Promise<R[]> {
    const {
      batchSize = 100,
      concurrency = 3,
      onProgress
    } = config;

    // 获取总数
    const total = await repository.count();
    const totalPages = Math.ceil(total / batchSize);

    const results: R[][] = [];

    // 分批处理
    for (let page = 0; page < totalPages; page += concurrency) {
      const batches = [];

      // 创建并发批次
      for (let i = 0; i < concurrency && page + i < totalPages; i++) {
        const currentPage = page + i;
        const skip = currentPage * batchSize;

        batches.push(
          repository.find({
            skip,
            take: batchSize
          }).then(items => ({
            items,
            page: currentPage
          }))
        );
      }

      // 等待批次完成
      const batchesData = await Promise.all(batches);

      // 处理每批数据
      for (const { items, page: currentPage } of batchesData) {
        if (items.length > 0) {
          const batchResult = await processFn(items);
          results.push(batchResult);

          // 进度回调
          if (onProgress) {
            onProgress((currentPage + 1) * batchSize, total);
          }
        }
      }
    }

    return results.flat();
  }

  /**
   * 游标方式处理大数据量
   */
  async processWithCursor<T, R>(
    repository: Repository<T>,
    where: any,
    processFn: (item: T) => Promise<R>,
    onProgress?: (current: number) => void
  ): Promise<R[]> {
    const results: R[] = [];
    let processed = 0;
    let hasMore = true;
    let lastId: string | null = null;
    const batchSize = 1000;

    while (hasMore) {
      // 构建查询条件
      const queryWhere = { ...where };
      if (lastId) {
        queryWhere['id'] = { $gt: lastId } as any;
      }

      // 查询一批数据
      const items = await repository.find({
        where: queryWhere,
        order: { id: 'ASC' as any },
        take: batchSize
      });

      if (items.length === 0) {
        hasMore = false;
        break;
      }

      // 处理每条数据
      for (const item of items) {
        const result = await processFn(item);
        results.push(result);
        processed++;
        lastId = item.id;
      }

      // 进度回调
      if (onProgress) {
        onProgress(processed);
      }

      // 如果获取数量少于批次大小，说明已到末尾
      if (items.length < batchSize) {
        hasMore = false;
      }
    }

    return results;
  }
}
```

##### 3.3.2 异步任务队列

```typescript
// src/modules/batch/services/queue.service.ts
import { Injectable } from '@nestjs/common';
import { Queue, Worker, Job } from 'bullmq';
import { Redis } from 'ioredis';

/**
 * 批量任务类型
 */
enum BatchJobType {
  IMPORT = 'batch_import',
  EXPORT = 'batch_export',
  SYNC = 'batch_sync'
}

/**
 * 批量任务服务
 */
@Injectable()
export class BatchQueueService {
  private importQueue: Queue;
  private exportQueue: Queue;
  private workers: Worker[] = [];

  constructor() {
    const connection = new Redis({
      host: process.env.REDIS_HOST,
      port: parseInt(process.env.REDIS_PORT),
      password: process.env.REDIS_PASSWORD
    });

    // 创建队列
    this.importQueue = new Queue('batch-import', { connection });
    this.exportQueue = new Queue('batch-export', { connection });

    // 启动Worker
    this.startWorkers(connection);
  }

  /**
   * 添加导入任务
   */
  async addImportJob(data: {
    userId: string;
    fileUrl: string;
    type: string;
  }): Promise<string> {
    const job = await this.importQueue.add(BatchJobType.IMPORT, data, {
      attempts: 3,
      backoff: {
        type: 'exponential',
        delay: 5000
      },
      removeOnComplete: {
        age: 24 * 3600 // 24小时后删除
      },
      removeOnFail: {
        age: 7 * 24 * 3600 // 7天后删除
      }
    });

    return job.id;
  }

  /**
   * 添加导出任务
   */
  async addExportJob(data: {
    userId: string;
    query: any;
    format: 'xlsx' | 'csv';
  }): Promise<string> {
    const job = await this.exportQueue.add(BatchJobType.EXPORT, data, {
      attempts: 2,
      timeout: 30 * 60 * 1000 // 30分钟超时
    });

    return job.id;
  }

  /**
   * 获取任务状态
   */
  async getJobStatus(queue: 'import' | 'export', jobId: string) {
    const targetQueue = queue === 'import' ? this.importQueue : this.exportQueue;
    const job = await targetQueue.getJob(jobId);

    if (!job) {
      return { status: 'not_found' };
    }

    const state = await job.getState();
    const progress = job.progress;

    return {
      status: state,
      progress,
      data: job.data,
      result: job.returnvalue,
      failedReason: job.failedReason
    };
  }

  /**
   * 启动Worker
   * @private
   */
  private startWorkers(connection: Redis) {
    // 导入Worker
    const importWorker = new Worker(
      'batch-import',
      async (job: Job) => {
        return this.processImportJob(job);
      },
      { connection, concurrency: 2 }
    );

    // 导出Worker
    const exportWorker = new Worker(
      'batch-export',
      async (job: Job) => {
        return this.processExportJob(job);
      },
      { connection, concurrency: 3 }
    );

    this.workers.push(importWorker, exportWorker);
  }

  /**
   * 处理导入任务
   * @private
   */
  private async processImportJob(job: Job) {
    const { userId, fileUrl, type } = job.data;

    // 更新进度
    job.updateProgress(10);

    try {
      // 下载文件
      const file = await this.downloadFile(fileUrl);
      job.updateProgress(30);

      // 解析并导入数据
      const result = await this.importFile(file, type, (progress) => {
        job.updateProgress(30 + progress * 0.6);
      });

      job.updateProgress(100);

      // 通知用户
      await this.notifyUser(userId, {
        type: 'import_complete',
        result
      });

      return result;

    } catch (error) {
      await this.notifyUser(userId, {
        type: 'import_failed',
        error: error.message
      });
      throw error;
    }
  }

  /**
   * 处理导出任务
   * @private
   */
  private async processExportJob(job: Job) {
    const { userId, query, format } = job.data;

    job.updateProgress(10);

    try {
      // 查询数据（分批）
      const data = await this.queryData(query, (progress) => {
        job.updateProgress(10 + progress * 0.7);
      });

      job.updateProgress(80);

      // 生成文件
      const fileUrl = await this.generateExportFile(data, format);
      job.updateProgress(100);

      // 通知用户
      await this.notifyUser(userId, {
        type: 'export_complete',
        fileUrl
      });

      return { fileUrl };

    } catch (error) {
      await this.notifyUser(userId, {
        type: 'export_failed',
        error: error.message
      });
      throw error;
    }
  }

  /**
   * 查询数据（分批）
   * @private
   */
  private async queryData(
    query: any,
    onProgress?: (progress: number) => void
  ): Promise<any[]> {
    // TODO: 实现分批查询逻辑
    return [];
  }

  /**
   * 生成导出文件
   * @private
   */
  private async generateExportFile(
    data: any[],
    format: string
  ): Promise<string> {
    // TODO: 生成文件并上传到OSS
    return 'https://example.com/export/file.xlsx';
  }

  /**
   * 下载文件
   * @private
   */
  private async downloadFile(url: string): Promise<Buffer> {
    // TODO: 实现文件下载
    return Buffer.from('');
  }

  /**
   * 导入文件
   * @private
   */
  private async importFile(
    file: Buffer,
    type: string,
    onProgress?: (progress: number) => void
  ): Promise<any> {
    // TODO: 实现文件导入
    return { success: 0, failed: 0 };
  }

  /**
   * 通知用户
   * @private
   */
  private async notifyUser(userId: string, notification: any): Promise<void> {
    // TODO: 发送站内信或邮件通知
  }
}
```

#### 3.4 数据库批量操作

```typescript
// src/modules/batch/services/database-batch.service.ts
import { Injectable } from '@nestjs/common';
import { DataSource, Repository } from 'typeorm';

/**
 * 数据库批量操作服务
 */
@Injectable()
export class DatabaseBatchService {
  constructor(private readonly dataSource: DataSource) {}

  /**
   * 批量插入（性能优化）
   */
  async batchInsert<T>(
    repository: Repository<T>,
    items: T[],
    batchSize = 500
  ): Promise<void> {
    const queryRunner = this.dataSource.createQueryRunner();
    await queryRunner.connect();
    await queryRunner.startTransaction();

    try {
      for (let i = 0; i < items.length; i += batchSize) {
        const batch = items.slice(i, i + batchSize);
        await queryRunner.manager.insert(repository.target, batch);
      }

      await queryRunner.commitTransaction();
    } catch (error) {
      await queryRunner.rollbackTransaction();
      throw error;
    } finally {
      await queryRunner.release();
    }
  }

  /**
   * 批量更新
   */
  async batchUpdate<T>(
    repository: Repository<T>,
    items: T[],
    batchSize = 200
  ): Promise<void> {
    const queryRunner = this.dataSource.createQueryRunner();
    await queryRunner.connect();
    await queryRunner.startTransaction();

    try {
      for (let i = 0; i < items.length; i += batchSize) {
        const batch = items.slice(i, i + batchSize);
        for (const item of batch) {
          await queryRunner.manager.save(repository.target, item);
        }
      }

      await queryRunner.commitTransaction();
    } catch (error) {
      await queryRunner.rollbackTransaction();
      throw error;
    } finally {
      await queryRunner.release();
    }
  }

  /**
   * 批量删除
   */
  async batchDelete<T>(
    repository: Repository<T>,
    ids: string[],
    batchSize = 500
  ): Promise<void> {
    for (let i = 0; i < ids.length; i += batchSize) {
      const batchIds = ids.slice(i, i + batchSize);
      await repository.delete(batchIds);
    }
  }

  /**
   * 批量Upsert（插入或更新）
   */
  async batchUpsert<T>(
    repository: Repository<T>,
    items: T[],
    conflictPaths: string[] = ['id'],
    batchSize = 500
  ): Promise<void> {
    const queryRunner = this.dataSource.createQueryRunner();
    await queryRunner.connect();
    await queryRunner.startTransaction();

    try {
      for (let i = 0; i < items.length; i += batchSize) {
        const batch = items.slice(i, i + batchSize);

        // 使用原生SQL进行upsert（PostgreSQL语法）
        const tableName = repository.metadata.tableName;
        const columns = repository.metadata.columns.map(c => c.databaseName);

        const values = batch.map(item =>
          `(${columns.map(col => {
            // @ts-ignore
            const value = item[col];
            return typeof value === 'string' ? `'${value}'` : value;
          }).join(',')})`
        ).join(',');

        const updateStr = columns
          .filter(c => c !== conflictPaths[0])
          .map(c => `${c} = EXCLUDED.${c}`)
          .join(',');

        const sql = `
          INSERT INTO ${tableName} (${columns.join(',')})
          VALUES ${values}
          ON CONFLICT (${conflictPaths.join(',')})
          DO UPDATE SET ${updateStr}
        `;

        await queryRunner.query(sql);
      }

      await queryRunner.commitTransaction();
    } catch (error) {
      await queryRunner.rollbackTransaction();
      throw error;
    } finally {
      await queryRunner.release();
    }
  }
}
```

### 4. 附录

#### 4.1 处理性能对比

| 数据量 | 直接处理 | 分批处理(500) | 游标处理 |
|--------|----------|---------------|----------|
| 1,000 | 2s | 1.5s | 2s |
| 10,000 | 45s | 12s | 15s |
| 100,000 | OOM | 120s | 150s |
| 1,000,000 | - | 25min | 30min |

#### 4.2 推荐配置

| 场景 | 批次大小 | 并发数 | 处理方式 |
|------|----------|--------|----------|
| 导入小文件(<1M) | 全量 | 1 | 同步 |
| 导入中文件(1-10M) | 500 | 1 | 同步 |
| 导入大文件(>10M) | 1000 | 2 | 异步队列 |
| 导出(<1W条) | 1000 | 3 | 同步 |
| 导出(>1W条) | 5000 | 3 | 异步队列 |

---

> 「***YanYuCloudCube***」
> 「***<admin@0379.email>***」
> 「***Words Initiate Quadrants, Language Serves as Core for the Future***」
> 「***All things converge in the cloud pivot; Deep stacks ignite a new era of intelligence***」
